{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importar Módulos Langchain\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp, CTransformers\n",
    "\n",
    "# Importar módulo para Front-End Web\n",
    "import gradio as gr\n",
    "\n",
    "# Importar módulo para carregar variáveis de ambiente do projeto\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatPDF:\n",
    "\n",
    "    def __init__(self, documents_folder: str, vectordb_folder: str, model_path: str, sentence_embedding_model: str, temperature: float = 0.1):\n",
    "        \"\"\"\n",
    "        Constrói todos os atributos necessários para o objeto ChatPDF\n",
    "\n",
    "        Args:\n",
    "            documents_folder (str): Caminho/Pasta com os documentos que serão carregados para conversação no Chat\n",
    "            vectordb_folder (str): Caminho/Pasta onde ficarão os arquivos do Chroma (Vector DB)\n",
    "            model_path (str): Caminho/Pasta que aponta para o modelo LLM a ser utilizado\n",
    "            sentence_embedding_model (str): Nome do modelo de Embedding que será usado para gerar os tokens dos documentos\n",
    "            temperature (float, optional): Temperatura para calibrar o nível de aleatoriedade das respostas. O padrão é 0.1 (Muito determinístico, pouco aleatório)\n",
    "        \"\"\"\n",
    "        self.documents_folder = documents_folder\n",
    "        self.vectordb_folder = vectordb_folder\n",
    "        self.model_path = model_path\n",
    "        self.sentence_embedding_model = sentence_embedding_model\n",
    "        self.temperature = temperature\n",
    "        self.pages = []\n",
    "        self.chunks = []\n",
    "\n",
    "    def load(self) -> int:\n",
    "        \"\"\"\n",
    "        Realiza a carga dos documentos do caminho/pasta definido no atributo documents_folder.\n",
    "\n",
    "        Returns:\n",
    "            int: Quantidade total de páginas carregadas de todos os arquivos PDF\n",
    "        \"\"\"\n",
    "\n",
    "        loader = DirectoryLoader(\n",
    "            self.documents_folder,\n",
    "            glob=\"**/*.pdf\",\n",
    "            loader_cls=PyMuPDFLoader,\n",
    "            show_progress=True,\n",
    "            use_multithreading=True\n",
    "        )\n",
    "\n",
    "        self.pages = loader.load()\n",
    "\n",
    "        return len(self.pages)\n",
    "    \n",
    "    def split(self, chunk_size: int = 1500, chunk_overlap: int = 150) -> int:\n",
    "        \"\"\"\n",
    "        Realiza o split das páginas em chunks para armazenar no Vector DB\n",
    "\n",
    "        Args:\n",
    "            chunk_size (int, optional): Quantidade máxima de caracteres de cada chunk. O padrão é 1500.\n",
    "            chunk_overlap (int, optional): Quantidade de caracteres de overlap entre chunks. O padrão é 150.\n",
    "\n",
    "        Returns:\n",
    "            int: Quantidade total de chunks de todas as páginas de todos os documentos carregados\n",
    "        \"\"\"\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        self.chunks = text_splitter.split_documents(self.pages)\n",
    "\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Obtem os embeddings do modelo de linguagem definido no atributo sentence_embedding_model.\n",
    "        \"\"\"\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=self.sentence_embedding_model)\n",
    "\n",
    "    def store(self):\n",
    "        \"\"\"\n",
    "        Armazena os chunks de todos os documentos no Vector DB, utilizando o embedding definido. \n",
    "        \"\"\"\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=self.chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.vectordb_folder\n",
    "        )\n",
    "\n",
    "        vectordb.persist()\n",
    "\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def create_llm(self):\n",
    "        \"\"\"\n",
    "        Cria uma LLM local, com base no modelo definido no atributo model_path.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.llm = LlamaCpp(model_path=self.model_path, verbose=True, n_ctx=2048, temperature=self.temperature)\n",
    "\n",
    "    def create_retriever(self):\n",
    "        \"\"\"\n",
    "        Cria um retriever de documentos com base do Vector DB já carregado com os documentos.\n",
    "        \"\"\"\n",
    "        self.retriever = self.vectordb.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "    def create_qa_session(self):\n",
    "        \"\"\"\n",
    "        Cria uma sessão de QA, usando o LLM e Retriever já instanciados.\n",
    "        \"\"\"\n",
    "\n",
    "        PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to male up an answer. Use three sentences maximum. Keep answer as concise as possible. Always say \"thanks for asking! at the end of the answer.\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Helpful Answer:\"\"\"\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "        self.qa = RetrievalQA.from_chain_type(\n",
    "            self.llm,\n",
    "            'stuff',\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={'prompt': QA_CHAIN_PROMPT}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as variáveis de ambiente do arquivo .env, para utilizar na hora de parametrizar o objeto ChatPDF\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = os.getenv('MODEL_PATH')\n",
    "VECTORDB_FOLDER = os.getenv('VECTORDB_FOLDER')\n",
    "DOCUMENTS_FOLDER = os.getenv('DOCUMENTS_FOLDER')\n",
    "SENTENCE_EMBEDDING_MODEL = os.getenv('SENTENCE_EMBEDDING_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto ChatPDF\n",
    "chat = ChatPDF(DOCUMENTS_FOLDER, VECTORDB_FOLDER, MODEL_PATH, SENTENCE_EMBEDDING_MODEL, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a carga dos documentos PDF que iremos interagir na sessão de Chat Q&A\n",
    "chat.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o split dos documentos carregados\n",
    "chat.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtem os embeddings do modelo de linguagem selecionado\n",
    "chat.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazena os chunks dos documentos, junto com o embedding, no Vector DB (Chroma)\n",
    "chat.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a LLM (LLAMA V2) localmente para interagirmos na sessão de chat\n",
    "chat.create_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o retriever, que irá recuperar os documentos do Vector DB, com base nos Prompts e usando a LLM\n",
    "chat.create_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma sessão de chat para Q&A, com base no LLM e Retriever\n",
    "chat.create_qa_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front-End da Aplicação Web - Gradio\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    def user(user_message, chat_history):\n",
    "        \n",
    "        # Retorna resposta da LLM, através da sessão de Q&A\n",
    "        result = chat.qa({\"query\": user_message})\n",
    "        \n",
    "        # Realiza um append na tela do chat, contendo a mensagem do usuário e a resposta do modelo\n",
    "        chat_history.append((user_message, result[\"result\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-gNinOHEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
